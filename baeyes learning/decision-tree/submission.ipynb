{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from graphviz import Digraph\n",
        "import argparse\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import copy"
      ],
      "metadata": {
        "id": "jqDMzaGokdtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_column_names(path):\n",
        "    '''\n",
        "        Gets the column names from the given path\n",
        "    '''\n",
        "    data = pd.read_csv(path)\n",
        "    return data.columns"
      ],
      "metadata": {
        "id": "FCdC_EZzbKnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_X_y(path):\n",
        "    \"\"\"\n",
        "    Loads the X and y from the given path.\n",
        "    Assumes last columns of the x are the target values.\n",
        "    :param path: the path to the x\n",
        "    :return: the x as X and y numpy arrays\n",
        "    \"\"\"\n",
        "    x = pd.read_csv(path)\n",
        "    X = x.drop(x.columns[-1], axis=1).to_numpy()\n",
        "    y = x[x.columns[-1]].to_numpy()\n",
        "    return X, y\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PRCoqpERajJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(X, y, train_size, shuffle=True, seed=42):\n",
        "    \"\"\"\n",
        "    Splits the x into training and test sets.\n",
        "    :param X: the x\n",
        "    :param y: the target values\n",
        "    :param train_size: the size of the training set\n",
        "    :param shuffle: whether to shuffle the x\n",
        "    :param seed: the seed for the random generator\n",
        "    :return: X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    # length = len(X)\n",
        "    # n_train = int(np.ceil(length*train_size))\n",
        "    # n_test = length - n_train\n",
        "\n",
        "    # if shuffle:\n",
        "    #     perm = np.random.RandomState(seed).permutation(length)\n",
        "    #     test_indices = perm[:n_test]\n",
        "    #     train_indices = perm[n_test:]\n",
        "    # else:\n",
        "    #     train_indices = np.arange(n_train)\n",
        "    #     test_indices = np.arange(n_train, length)\n",
        "\n",
        "    # X_train = X[train_indices]\n",
        "    # X_test = X[test_indices]\n",
        "    # y_train = y[train_indices]\n",
        "    # y_test = y[test_indices]\n",
        "    X_train, X_test, y_train, y_test = tts(\n",
        "        X, y, stratify=y, test_size=1-train_size, random_state=seed)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iUU-2eXdamQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_test_split(X, y, train_size, val_size, shuffle=True, seed=42):\n",
        "    '''\n",
        "    Splits the x into training, validation and test sets.\n",
        "    :param X: the x\n",
        "    :param y: the target values\n",
        "    :param train_size: the size of the training set\n",
        "    :param val_size: the size of the validation set\n",
        "    :param shuffle: whether to shuffle the x\n",
        "    :param seed: the seed for the random generator\n",
        "    :return: X_train, X_val, X_test, y_train, y_val, y_test\n",
        "    '''\n",
        "    # length = len(X)\n",
        "    # n_train = int(np.ceil(length*train_size))\n",
        "    # n_val = int(np.ceil(length*val_size))\n",
        "    # n_test = length - n_train - n_val\n",
        "\n",
        "    # if shuffle:\n",
        "    #     perm = np.random.RandomState(seed).permutation(length)\n",
        "    #     test_indices = perm[:n_test]\n",
        "    #     val_indices = perm[n_test:n_test+n_val]\n",
        "    #     train_indices = perm[n_test+n_val:]\n",
        "    # else:\n",
        "    #     train_indices = np.arange(n_train)\n",
        "    #     val_indices = np.arange(n_train, n_train + n_val)\n",
        "    #     test_indices = np.arange(n_train + n_val, length)\n",
        "\n",
        "    # X_train = X[train_indices]\n",
        "    # X_val = X[val_indices]\n",
        "    # X_test = X[test_indices]\n",
        "    # y_train = y[train_indices]\n",
        "    # y_val = y[val_indices]\n",
        "    # y_test = y[test_indices]\n",
        "\n",
        "    test_size = 1-train_size-val_size\n",
        "    X_train, X_temp, y_train, y_temp = tts(\n",
        "        X, y, stratify=y, test_size=(1.0 - train_size), random_state=seed)\n",
        "    relative_test_size = test_size / (val_size + test_size)\n",
        "    X_val, X_test, y_val, y_test = tts(\n",
        "        X_temp, y_temp, stratify=y_temp, test_size=relative_test_size, random_state=seed)\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v3nu70Hgapwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_purity(y):\n",
        "    \"\"\"\n",
        "    Checks if the given array is pure.\n",
        "    :param y: the array\n",
        "    :return: True if the array is pure, False otherwise\n",
        "    \"\"\"\n",
        "    return len(set(y)) == 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PH7XzefBateL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_array(y):\n",
        "    \"\"\"\n",
        "    Classifies the array into a single class.\n",
        "    find most common number and return that\n",
        "    :param y: the array\n",
        "    :return: the class\n",
        "    \"\"\"\n",
        "    classes, counts = np.unique(y, return_counts=True)\n",
        "    return classes[counts.argmax()]\n",
        "    # return np.argmax(np.bincount(y.astype(int)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2JaKYC2yawYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_possible_breaks(X, type_arr):\n",
        "    '''\n",
        "        Calculates possible breaks for a given set of features \n",
        "    '''\n",
        "    breaks = {}\n",
        "    for col_idx in range(X.shape[1]):\n",
        "        unique_vals = np.unique(X[:, col_idx])\n",
        "        num_vals = np.unique(X[:, col_idx]).shape[0]\n",
        "\n",
        "        type = type_arr[col_idx]\n",
        "        if type == \"cont\":\n",
        "            breaks[col_idx] = []\n",
        "            for i in range(1, num_vals):\n",
        "                current_value = unique_vals[i]\n",
        "                previous_value = unique_vals[i - 1]\n",
        "                potential_split = (current_value + previous_value) / 2\n",
        "                breaks[col_idx].append(potential_split)\n",
        "        elif num_vals > 1:\n",
        "            breaks[col_idx] = unique_vals\n",
        "    return breaks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IyCMH-w3a1fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_children_np(X, y, col_idx, col_val, type_arr):\n",
        "    '''\n",
        "        Creates the children of a dataset given split column and value\n",
        "    '''\n",
        "    y = y.reshape(-1, 1)\n",
        "    X_n = np.hstack((X, y))\n",
        "    relevant_column = X_n[:, col_idx]\n",
        "    # print(relevant_column)\n",
        "    # print(relevant_column<=col_val)\n",
        "    if type_arr[col_idx] == \"cont\":\n",
        "        X_one = X_n[relevant_column <= col_val]\n",
        "        X_two = X_n[relevant_column > col_val]\n",
        "    else:\n",
        "        X_one = X_n[relevant_column == col_val]\n",
        "        X_two = X_n[relevant_column != col_val]\n",
        "\n",
        "    # print(X_one.shape, X_two.shape)\n",
        "    Y_one = X_one[:, -1]\n",
        "    Y_two = X_two[:, -1]\n",
        "    X_one = X_one[:, :-1]\n",
        "    X_two = X_two[:, :-1]\n",
        "\n",
        "    # print(X_one.shape, X_two.shape, Y_one.shape, Y_two.shape)\n",
        "    return X_one, Y_one, X_two, Y_two\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hU0CUj--a3rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_entropy_np(y):\n",
        "    \"\"\"\n",
        "    Calculates the entropy of the given array.\n",
        "    :param y: the array\n",
        "    :return: the entropy\n",
        "    \"\"\"\n",
        "    classes, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts / counts.sum()\n",
        "    return np.sum(probabilities * -np.log2(probabilities))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5JTtPBUDa6Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_info_gain(X, y, col_idx, col_val, type_arr):\n",
        "    '''\n",
        "        Calculates the information gain of a given split\n",
        "    '''\n",
        "    X_one, Y_one, X_two, Y_two = create_children_np(\n",
        "        X, y, col_idx, col_val, type_arr)\n",
        "    p = len(X_one) / len(X)\n",
        "    return calc_entropy_np(y) - (p * calc_entropy_np(Y_one) + (1 - p) * calc_entropy_np(Y_two))\n",
        "\n"
      ],
      "metadata": {
        "id": "wI3X8KKXa9SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_split(X, y, type_arr, method=\"entropy\"):\n",
        "    '''\n",
        "        Calculates the best split for a given set of features\n",
        "    '''\n",
        "    best_col = -1\n",
        "    best_val = -1\n",
        "    best_gain = -10000\n",
        "    breaks = get_possible_breaks(X, type_arr)\n",
        "    for col_idx in breaks:\n",
        "        for col_val in breaks[col_idx]:\n",
        "            if method == \"entropy\":\n",
        "                gain = calc_info_gain(X, y, col_idx, col_val, type_arr)\n",
        "            else:\n",
        "                gain = calc_gini_gain(X, y, col_idx, col_val, type_arr)\n",
        "            if gain > best_gain:\n",
        "                best_col = col_idx\n",
        "                best_val = col_val\n",
        "                best_gain = gain\n",
        "    return best_col, best_val\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0gihT5yia_iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_feature_type(X, cont_thresh):\n",
        "    '''\n",
        "        Assigns the type of each feature based on the data\n",
        "    '''\n",
        "    type_arr = []\n",
        "    for col_idx in range(X.shape[1]):\n",
        "        type_val = X[:, col_idx][0]\n",
        "        unique_vals = np.unique(X[:, col_idx])\n",
        "        if len(unique_vals) < cont_thresh or isinstance(type_val, str):\n",
        "            type_arr.append(\"discrete\")\n",
        "        else:\n",
        "            type_arr.append(\"cont\")\n",
        "    return type_arr\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZTV8kxqrbBkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(y_true, y_pred):\n",
        "    '''\n",
        "        Calculates the accuracy of the prediction\n",
        "    '''\n",
        "    return np.sum(y_pred == y_true) / len(y_pred)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6hsGbep_bDhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter(X, y, col_idx, col_val, type_arr):\n",
        "\n",
        "    y = y.reshape(-1, 1)\n",
        "    X_n = np.hstack((X, y))\n",
        "    relevant_column = X_n[:, col_idx]\n",
        "\n",
        "    if type_arr[col_idx] == \"cont\":\n",
        "        X_yes = X_n[relevant_column <= col_val]\n",
        "        X_no = X_n[relevant_column > col_val]\n",
        "\n",
        "    else:\n",
        "        X_yes = X_n[relevant_column == col_val]\n",
        "        X_no = X_n[relevant_column != col_val]\n",
        "\n",
        "    Y_yes = X_yes[:, -1]\n",
        "    Y_no = X_no[:, -1]\n",
        "    X_yes = X_yes[:, :-1]\n",
        "    X_no = X_no[:, :-1]\n",
        "    return X_yes, Y_yes, X_no, Y_no\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3SAY4AoZbE4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_node(X, y):\n",
        "    return classify_array(y)"
      ],
      "metadata": {
        "id": "yNCvN8E7bGRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GLOBAL_COUNT=0\n",
        "class Node:\n",
        "    '''\n",
        "        Class defines the nodes of the decision tree\n",
        "        self.attr: [String] attribute of the node\n",
        "        self.val: [Float] value of the attribute\n",
        "        self.avg_attr: [Float] average of the attribute\n",
        "\n",
        "        self.left: [Node] left child of the node\n",
        "        self.right: [Node] right child of the node\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self, attribute, value, type_arr):\n",
        "        '''\n",
        "            Initializes the node\n",
        "        '''\n",
        "        global GLOBAL_COUNT\n",
        "        GLOBAL_COUNT+=1\n",
        "        self.node_id=GLOBAL_COUNT\n",
        "        \n",
        "        self.attr_idx = attribute\n",
        "        self.val = value\n",
        "        self.attr_type = type_arr[self.attr_idx]\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.leaf = False\n",
        "        self.classification = None\n",
        "\n",
        "    def make_leaf(self, classification):\n",
        "        '''\n",
        "            Makes the node a leaf node\n",
        "        '''\n",
        "        # print(f'Making leaf node {classification}')\n",
        "        self.leaf = True\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.classification = classification\n",
        "\n",
        "    def get_classification(self):\n",
        "        '''\n",
        "            Returns the classification of the node\n",
        "        '''\n",
        "        return self.classification\n",
        "\n",
        "    def predict_node(self, X):\n",
        "        '''\n",
        "            Predicts the class of the instance X\n",
        "        '''\n",
        "        if self.leaf:\n",
        "            return self.classification\n",
        "        else:\n",
        "            if self.attr_type == 'cont':\n",
        "                if X[self.attr_idx] <= self.val:\n",
        "                    return self.left.predict_node(X)\n",
        "                else:\n",
        "                    return self.right.predict_node(X)\n",
        "            else:\n",
        "                if X[self.attr_idx] == self.val:\n",
        "                    return self.left.predict_node(X)\n",
        "                else:\n",
        "                    return self.right.predict_node(X)\n",
        "\n",
        "    def __eq__(self, other) -> bool:\n",
        "        # print(self,other)\n",
        "        if other is None:\n",
        "            return False\n",
        "        if self.leaf and other.leaf:\n",
        "            return self.classification == other.classification\n",
        "        if self.attr_idx != other.attr_idx:\n",
        "            return False\n",
        "        if self.val != other.val:\n",
        "            return False\n",
        "        if self.attr_type != other.attr_type:\n",
        "            return False\n",
        "        if self.left != other.left:\n",
        "            return False\n",
        "        if self.right != other.right:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def dfs_count(self):\n",
        "        ans = 1\n",
        "        if not self.left is None:\n",
        "            ans += self.left.dfs_count()\n",
        "        if not self.right is None:\n",
        "            ans += self.right.dfs_count()\n",
        "        return ans\n",
        "\n",
        "    def calc_prune_error(self, X_val, y_val):\n",
        "        preds = np.array([self.predict_node(x) for x in X_val])\n",
        "        return np.sum(preds != y_val)\n",
        "\n",
        "    def prune_base(self, y_train, X_val, y_val, n_node):\n",
        "\n",
        "        leaf = classify_array(y_train)\n",
        "        errors_leaf = np.sum(y_val != leaf)\n",
        "        errors_node = np.sum(y_val != np.array(\n",
        "            [self.predict_node(x) for x in X_val]))\n",
        "\n",
        "        if errors_leaf <= errors_node:\n",
        "            n_node.make_leaf(leaf)\n",
        "        \n",
        "\n",
        "    def prune_rec(self, X_train, y_train, X_val, y_val,type_arr):\n",
        "\n",
        "        n_node = Node(self.attr_idx, self.val, type_arr)\n",
        "        n_node.leaf = self.leaf\n",
        "        n_node.classification = self.classification\n",
        "        n_node.left = self.left\n",
        "        n_node.right = self.right\n",
        "        # print(f\"start {type(n_node)}\")\n",
        "        if self.leaf:\n",
        "            n_node = self.prune_base(y_train, X_val, y_val, n_node)\n",
        "\n",
        "        else:\n",
        "            X_train_yes, Y_train_yes, X_train_no, Y_train_no = filter(\n",
        "                X_train, y_train, self.attr_idx, self.val,type_arr)\n",
        "            X_val_yes, Y_val_yes, X_val_no, Y_val_no = filter(\n",
        "                X_val, y_val, self.attr_idx, self.val,type_arr)\n",
        "\n",
        "            if not (self.left is None or self.left.leaf==True):\n",
        "                n_node.left = self.left.prune_rec(X_train_yes,Y_train_yes, X_val_yes, Y_val_yes,type_arr)\n",
        "            if not (self.right is None or self.right.leaf==True):\n",
        "                n_node.right = self.right.prune_rec(X_train_no,Y_train_no, X_val_no, Y_val_no,type_arr)\n",
        "\n",
        "            self.prune_base(y_train, X_val, y_val, n_node)\n",
        "\n",
        "        # print(f\"end {type(n_node)}\")\n",
        "        return n_node\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SldgOHKGknnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    '''\n",
        "        Class defines the decision tree\n",
        "        self.root: [Node] root of the tree\n",
        "        self.X:  [Nest List] training features\n",
        "        self.y:  [List] training labels\n",
        "    '''\n",
        "\n",
        "    def __init__(self, X, y, column_names, min_leaf_size, max_depth,impurity_measure):\n",
        "        '''\n",
        "            Initializes the tree\n",
        "        '''\n",
        "        self.measure=impurity_measure\n",
        "        self.root = None\n",
        "        self.root_pruned = None\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.min_leaf_size = min_leaf_size\n",
        "        self.max_depth = max_depth\n",
        "        self.column_names = column_names\n",
        "        self.type_arr = assign_feature_type(X, 2)\n",
        "\n",
        "    def fit(self):\n",
        "        '''\n",
        "                Builds the decision tree\n",
        "        '''\n",
        "        global GLOBAL_COUNT\n",
        "        GLOBAL_COUNT=0\n",
        "        self.root = self.build_tree(self.X, self.y)\n",
        "\n",
        "    def is_leaf(self, X_lo, y_lo):\n",
        "        '''\n",
        "            Checks if the node is a leaf\n",
        "            node: [Node] node to be checked\n",
        "        '''\n",
        "        if X_lo.shape[0] <= self.min_leaf_size:\n",
        "            return True\n",
        "        if check_purity(y_lo):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def build_tree(self, X, y, depth=0):\n",
        "        '''\n",
        "            Recursively builds the decision tree\n",
        "            node: [Node] node to be built\n",
        "            depth: [Int] depth of the node\n",
        "        '''\n",
        "\n",
        "        \n",
        "        if self.is_leaf(X, y) or depth == self.max_depth:\n",
        "            node = Node(0, 0, self.type_arr)\n",
        "            node.make_leaf(classify_array(y))\n",
        "            return node\n",
        "        else:\n",
        "            depth += 1\n",
        "            best_attr, best_val = get_best_split(\n",
        "                X, y, self.type_arr,self.measure)\n",
        "            node = Node(best_attr, best_val, self.type_arr)\n",
        "            \n",
        "            X_left, y_left, X_right, y_right = create_children_np(\n",
        "                X, y, best_attr, best_val, self.type_arr)\n",
        "            \n",
        "            if X_left.shape[0] == 0 or X_right.shape[0] == 0:\n",
        "                node.make_leaf(classify_array(y))    \n",
        "                return node\n",
        "\n",
        "            left_tree = self.build_tree(X_left, y_left, depth)\n",
        "            right_tree = self.build_tree(X_right, y_right, depth)\n",
        "            \n",
        "            # print(\"hii\",left_tree,right_tree)\n",
        "            if left_tree == right_tree:    \n",
        "                node.make_leaf(classify_array(y_left))\n",
        "            else:\n",
        "                node.leaf = False\n",
        "                node.left = left_tree\n",
        "                node.right = right_tree\n",
        "\n",
        "            return node\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "            Predicts the labels of the test data\n",
        "            X: [Nest List] test features\n",
        "        '''\n",
        "        if self.root is None:\n",
        "            return None\n",
        "        else:\n",
        "            return np.array([self.root.predict_node(x) for x in X])\n",
        "\n",
        "    def pruned_predict(self, X):\n",
        "        if self.root_pruned is None:\n",
        "            return None\n",
        "        else:\n",
        "            return np.array([self.root_pruned.predict_node(x) for x in X])\n",
        "\n",
        "    def calc_accuracy(self, X, y,print_report=True):\n",
        "        '''\n",
        "            Calculates the accuracy of the decision tree\n",
        "            X: [Nest List] test features\n",
        "            y: [List] test labels\n",
        "        '''\n",
        "        y_pred = self.predict(X)\n",
        "        from sklearn.metrics import classification_report\n",
        "        if print_report:\n",
        "            print(classification_report(y, y_pred))\n",
        "        return calc_accuracy(y, y_pred)\n",
        "\n",
        "    def calc_pruned_accuracy(self, X, y,print_report=True):\n",
        "        y_pred = self.pruned_predict(X)\n",
        "        from sklearn.metrics import classification_report\n",
        "        if print_report:\n",
        "            print(classification_report(y, y_pred))\n",
        "        return calc_accuracy(y, y_pred)\n",
        "\n",
        "    def print_tree(self, node=None, depth=0):\n",
        "        '''\n",
        "            Prints the tree in a readable format\n",
        "            node: [Node] node to be printed\n",
        "            depth: [Int] depth of the node\n",
        "        '''\n",
        "\n",
        "        if node.leaf:\n",
        "            print('|'*depth+'Leaf: '+str(node.classification))\n",
        "        else:\n",
        "            compoperator = '<='\n",
        "            if node.attr_type == 'discrete':\n",
        "                compoperator = '=='\n",
        "            print('|'*depth+'Attribute: ' +\n",
        "                  self.column_names[node.attr_idx]+'  ' + compoperator + '  Value: '+str(node.val))\n",
        "            self.print_tree(node.left, depth+1)\n",
        "            self.print_tree(node.right, depth+1)\n",
        "\n",
        "    def count_nodes(self):\n",
        "        return self.root.dfs_count()\n",
        "\n",
        "    def post_prune(self, X_train, y_train, X_val, y_val):\n",
        "        '''\n",
        "            Recursively prunes the tree\n",
        "        '''\n",
        "        global GLOBAL_COUNT\n",
        "        GLOBAL_COUNT=0\n",
        "        self.root_pruned = self.root.prune_rec(X_train, y_train, X_val, y_val,self.type_arr)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FqA-WKlkcDej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_node(vertex, feature_names, count):\n",
        "    if vertex.leaf:\n",
        "        return f'ID {vertex.node_id},\\nClassification -> {vertex.classification}\\n'\n",
        "    return f'ID {vertex.node_id}\\n{feature_names[vertex.attr_idx]} <= {vertex.val}\\n'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WDM7WnzmcHsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tree_to_gv(node_root, feature_names,file_name=\"decision_tree.gv\"):\n",
        "    f = Digraph('Decision Tree', filename=file_name)\n",
        "    # f.attr(rankdir='LR', size='1000,500')\n",
        "\n",
        "    f.attr('node', shape='rectangle')\n",
        "    q = [node_root]\n",
        "    idx = 0\n",
        "    while len(q) > 0:\n",
        "        node = q.pop(0)\n",
        "        if node is None:\n",
        "            continue\n",
        "        if not node.left is None:\n",
        "            f.edge(render_node(node, feature_names, idx), render_node(\n",
        "                node.left, feature_names, idx), label='True')\n",
        "            idx += 1\n",
        "            q.append(node.left)\n",
        "        if not node.right is None:\n",
        "            f.edge(render_node(node, feature_names, idx), render_node(\n",
        "                node.right, feature_names, idx), label='False')\n",
        "            idx += 1\n",
        "            q.append(node.right)\n",
        "    f.render(f'./{file_name}', view=True)"
      ],
      "metadata": {
        "id": "zUAJ24h0cI5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_best_tree(X_full, y_full, measure):\n",
        "    X_b_train = None\n",
        "    X_b_test = None\n",
        "    X_b_val = None\n",
        "    y_b_train = None\n",
        "    y_b_test = None\n",
        "    y_b_val = None\n",
        "    best_acc = -1\n",
        "    acc_list=[]\n",
        "    acc_train_list=[]\n",
        "    for i in range(1, 11):\n",
        "        X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
        "            X_full, y_full, 0.6, 0.2, seed=i+RANDOM_SEED)\n",
        "        Tree = DecisionTree(\n",
        "            X_train, y_train, feature_names, MIN_LEAF_SIZE, MAX_HEIGHT, measure)\n",
        "        Tree.fit()\n",
        "        train_acc = Tree.calc_accuracy(X_train, y_train, print_report=False)\n",
        "        test_acc = Tree.calc_accuracy(X_test, y_test, print_report=False)\n",
        "        print(f\"Training on split {i} complete\")\n",
        "        print(f\"Training accuracy: {train_acc}\")\n",
        "        print(f\"Testing accuracy: {test_acc}\")\n",
        "        acc_list.append(test_acc)\n",
        "        acc_train_list.append(train_acc)\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            X_b_train = X_train\n",
        "            X_b_test = X_test\n",
        "            y_b_train = y_train\n",
        "            y_b_test = y_test\n",
        "            X_b_val = X_val\n",
        "            y_b_val = y_val\n",
        "            best_tree=copy.deepcopy(Tree)\n",
        "    # best_tree=tree.DecisionTree(X_b_train, y_b_train, feature_names, MIN_LEAF_SIZE, MAX_HEIGHT, measure)\n",
        "    # best_tree.fit()\n",
        "    print(\"\\n\\n\")\n",
        "    print(f\"Average train accuracy over 10 test train splits is {np.mean(acc_train_list)}\")\n",
        "    print(f\"Average test acuracy over 10 test train splits is {np.mean(acc_list)}\")\n",
        "    return best_tree, X_b_train, X_b_test, X_b_val, y_b_train, y_b_test, y_b_val\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kY1eKemvcghL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def height_ablation(X_train, y_train,X_test,y_test ,X_val, y_val, measure):\n",
        "    test_acc_depth_list = []\n",
        "    train_acc_depth_list=[]\n",
        "    val_acc_depth_list=[]\n",
        "    num_node_list = []\n",
        "\n",
        "    for i in range(1, 25):\n",
        "        print(f\"Checking Height = {i}\")\n",
        "        Tree_3 = DecisionTree(\n",
        "            X_train, y_train, feature_names, MIN_LEAF_SIZE, i, measure)\n",
        "        Tree_3.fit()\n",
        "        test_acc_depth_list.append(Tree_3.calc_accuracy(X_test, y_test, print_report=False))\n",
        "        train_acc_depth_list.append(Tree_3.calc_accuracy(X_train, y_train, print_report=False))\n",
        "        val_acc_depth_list.append(Tree_3.calc_accuracy(X_val, y_val, print_report=False))\n",
        "        num_node_list.append(Tree_3.count_nodes())\n",
        "\n",
        "    plt.plot(range(1, 25), test_acc_depth_list, label=\"Test\")\n",
        "    plt.plot(range(1, 25), train_acc_depth_list, label=\"Train\")\n",
        "    plt.plot(range(1, 25), val_acc_depth_list, label=\"Validation\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Depth\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy vs Depth\")\n",
        "    plt.show()\n",
        "\n",
        "    Optimal_depth = 1+np.argmax(np.array(val_acc_depth_list))\n",
        "    Best_tree = DecisionTree(\n",
        "        X_train, y_train, feature_names, MIN_LEAF_SIZE, Optimal_depth, measure)\n",
        "    Best_tree.fit()\n",
        "    print(f\"Optimal depth: {Optimal_depth}\")\n",
        "    return Best_tree\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qg8hFK_UclkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_entropy_exp(X_full,y_full,X_train_,y_train_,X_test_,y_test_):\n",
        "    print(\"\\n\"+\"-\"*50 + \"TREE with Entropy\" + \"-\"*50)\n",
        "    MEASURE = \"entropy\"\n",
        "    start = time.time()\n",
        "    Tree_1_entropy = DecisionTree(\n",
        "        X_train_, y_train_, feature_names, MIN_LEAF_SIZE, MAX_HEIGHT, MEASURE)\n",
        "    Tree_1_entropy.fit()\n",
        "    end = time.time()\n",
        "    print(\"Training complete\")\n",
        "    print(\"Training accuracy:\", Tree_1_entropy.calc_accuracy(X_train_, y_train_))\n",
        "    print(\"Testing accuracy:\", Tree_1_entropy.calc_accuracy(X_test_, y_test_))\n",
        "    print(\"Time taken:\", end - start)\n",
        "\n",
        "    print(\"\\n\"+\"-\"*50 + \"TREE over 10 random splits ENTROPY\" + \"-\"*50)\n",
        "    BEST_TREE, X_train, X_test, X_val, y_train, y_test,y_val = select_best_tree(X_full, y_full, \"entropy\")\n",
        "    print(\"\\n\"+\"-\"*50 + \"BEST TREE OVER 10 RANDOM SPLITS ENTROPY\" + \"-\"*50)\n",
        "    print(\"Training accuracy:\", BEST_TREE.calc_accuracy(X_train, y_train))\n",
        "    print(\"Testing accuracy:\", BEST_TREE.calc_accuracy(X_test, y_test))\n",
        "    tree_to_gv(BEST_TREE.root, feature_names, \"unprunedDT.gv\")\n",
        "\n",
        "    print(\"\\n\"+\"-\"*50 + \"DEPTH Vs Accuracy ENTROPY\" + \"-\"*50)\n",
        "    BEST_TREE_HEIGHT = height_ablation(\n",
        "        X_train, y_train, X_test, y_test,X_val,y_val, \"entropy\")\n",
        "\n",
        "    print(\"\\n\"+\"-\"*50+\"PRUNING OPERATIONS\"+\"-\"*50)\n",
        "    print(\"X_train:\", X_train.shape)\n",
        "    print(\"X_test:\", X_test.shape)\n",
        "    print(\"X_val:\", X_val.shape)\n",
        "    print(\"y_train:\", y_train.shape)\n",
        "    print(\"y_test:\", y_test.shape)\n",
        "    print(\"y_val:\", y_val.shape)\n",
        "    print(\"Unpruned best tree accuracies:\")\n",
        "    print(\"Training accuracy:\", BEST_TREE.calc_accuracy(X_train, y_train))\n",
        "    print(\"Testing accuracy:\", BEST_TREE.calc_accuracy(X_test, y_test))\n",
        "    print(\"Validation accuracy:\", BEST_TREE.calc_accuracy(X_val, y_val))\n",
        "\n",
        "    BEST_TREE.post_prune(X_train, y_train, X_val, y_val)\n",
        "    print(\"\\n\"+\"-\"*50+\"Post Pruning complete\"+\"-\"*50)\n",
        "    print(\"Training accuracy:\", BEST_TREE.calc_pruned_accuracy(X_train, y_train))\n",
        "    print(\"Testing accuracy:\", BEST_TREE.calc_pruned_accuracy(X_test, y_test))\n",
        "    print(\"Validation accuracy:\", BEST_TREE.calc_pruned_accuracy(X_val, y_val))\n",
        "    tree_to_gv(BEST_TREE.root_pruned, feature_names, \"prunedDT.gv\")\n",
        "    tree_to_gv(BEST_TREE_HEIGHT.root, feature_names,\"unpruned_best_tree_optimal_height_entropy.gv\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bexor8IscpdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_HEIGHT = 10\n",
        "DATA_PATH = \"/content/Dataset_E.csv\"\n",
        "MIN_LEAF_SIZE = 1 \n",
        "MEASURE = \"entropy\"\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "print(f\"MAX DEPTH = {MAX_HEIGHT}\")\n",
        "print(f\"DATA PATH = {DATA_PATH}\")\n",
        "\n",
        "if (MAX_HEIGHT < 0):\n",
        "    MAX_HEIGHT = 10\n",
        "\n",
        "print(\"-\"*50 + \"PREPROCESSING\" + \"-\"*50)\n",
        "feature_names = get_column_names(DATA_PATH)\n",
        "X_full, y_full = get_X_y(DATA_PATH)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_full, y_full, 0.8)\n",
        "print(\"X_train:\", X_train.shape)\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"y_test:\", y_test.shape)\n",
        "\n",
        "run_entropy_exp(X_full=X_full, y_full=y_full,X_train_=X_train,y_train_=y_train,X_test_=X_test,y_test_=y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CkwO3-bMcslZ",
        "outputId": "3f9f18b6-feca-4886-8f9b-eb2afdfb801c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAX DEPTH = 10\n",
            "DATA PATH = /content/Dataset_E.csv\n",
            "--------------------------------------------------PREPROCESSING--------------------------------------------------\n",
            "X_train: (614, 8)\n",
            "X_test: (154, 8)\n",
            "y_train: (614,)\n",
            "y_test: (154,)\n",
            "\n",
            "--------------------------------------------------TREE with Entropy--------------------------------------------------\n",
            "Training complete\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.97       400\n",
            "           1       0.96      0.91      0.93       214\n",
            "\n",
            "    accuracy                           0.95       614\n",
            "   macro avg       0.95      0.94      0.95       614\n",
            "weighted avg       0.95      0.95      0.95       614\n",
            "\n",
            "Training accuracy: 0.9543973941368078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.80      0.76       100\n",
            "           1       0.53      0.43      0.47        54\n",
            "\n",
            "    accuracy                           0.67       154\n",
            "   macro avg       0.63      0.61      0.62       154\n",
            "weighted avg       0.66      0.67      0.66       154\n",
            "\n",
            "Testing accuracy: 0.6688311688311688\n",
            "Time taken: 2.2368295192718506\n",
            "\n",
            "--------------------------------------------------TREE over 10 random splits ENTROPY--------------------------------------------------\n",
            "Training on split 1 complete\n",
            "Training accuracy: 0.9565217391304348\n",
            "Testing accuracy: 0.7402597402597403\n",
            "Training on split 2 complete\n",
            "Training accuracy: 0.9347826086956522\n",
            "Testing accuracy: 0.6753246753246753\n",
            "Training on split 3 complete\n",
            "Training accuracy: 0.9760869565217392\n",
            "Testing accuracy: 0.6818181818181818\n",
            "Training on split 4 complete\n",
            "Training accuracy: 0.9739130434782609\n",
            "Testing accuracy: 0.7207792207792207\n",
            "Training on split 5 complete\n",
            "Training accuracy: 0.9543478260869566\n",
            "Testing accuracy: 0.7532467532467533\n",
            "Training on split 6 complete\n",
            "Training accuracy: 0.9608695652173913\n",
            "Testing accuracy: 0.6818181818181818\n",
            "Training on split 7 complete\n",
            "Training accuracy: 0.9760869565217392\n",
            "Testing accuracy: 0.6753246753246753\n",
            "Training on split 8 complete\n",
            "Training accuracy: 0.941304347826087\n",
            "Testing accuracy: 0.7532467532467533\n",
            "Training on split 9 complete\n",
            "Training accuracy: 0.9891304347826086\n",
            "Testing accuracy: 0.7272727272727273\n",
            "Training on split 10 complete\n",
            "Training accuracy: 0.9608695652173913\n",
            "Testing accuracy: 0.7012987012987013\n",
            "\n",
            "\n",
            "\n",
            "Average train accuracy over 10 test train splits is 0.962391304347826\n",
            "Average test acuracy over 10 test train splits is 0.711038961038961\n",
            "\n",
            "--------------------------------------------------BEST TREE OVER 10 RANDOM SPLITS ENTROPY--------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.97       299\n",
            "           1       0.96      0.91      0.93       161\n",
            "\n",
            "    accuracy                           0.95       460\n",
            "   macro avg       0.96      0.94      0.95       460\n",
            "weighted avg       0.95      0.95      0.95       460\n",
            "\n",
            "Training accuracy: 0.9543478260869566\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.80      0.81       100\n",
            "           1       0.64      0.67      0.65        54\n",
            "\n",
            "    accuracy                           0.75       154\n",
            "   macro avg       0.73      0.73      0.73       154\n",
            "weighted avg       0.76      0.75      0.75       154\n",
            "\n",
            "Testing accuracy: 0.7532467532467533\n",
            "\n",
            "--------------------------------------------------DEPTH Vs Accuracy ENTROPY--------------------------------------------------\n",
            "Checking Height = 1\n",
            "Checking Height = 2\n",
            "Checking Height = 3\n",
            "Checking Height = 4\n",
            "Checking Height = 5\n",
            "Checking Height = 6\n",
            "Checking Height = 7\n",
            "Checking Height = 8\n",
            "Checking Height = 9\n",
            "Checking Height = 10\n",
            "Checking Height = 11\n",
            "Checking Height = 12\n",
            "Checking Height = 13\n",
            "Checking Height = 14\n",
            "Checking Height = 15\n",
            "Checking Height = 16\n",
            "Checking Height = 17\n",
            "Checking Height = 18\n",
            "Checking Height = 19\n",
            "Checking Height = 20\n",
            "Checking Height = 21\n",
            "Checking Height = 22\n",
            "Checking Height = 23\n",
            "Checking Height = 24\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal depth: 3\n",
            "Number of nodes: 9\n",
            "\n",
            "--------------------------------------------------PRUNING OPERATIONS--------------------------------------------------\n",
            "X_train: (460, 8)\n",
            "X_test: (154, 8)\n",
            "X_val: (154, 8)\n",
            "y_train: (460,)\n",
            "y_test: (154,)\n",
            "y_val: (154,)\n",
            "Unpruned best tree accuracies:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.97       299\n",
            "           1       0.96      0.91      0.93       161\n",
            "\n",
            "    accuracy                           0.95       460\n",
            "   macro avg       0.96      0.94      0.95       460\n",
            "weighted avg       0.95      0.95      0.95       460\n",
            "\n",
            "Training accuracy: 0.9543478260869566\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.80      0.81       100\n",
            "           1       0.64      0.67      0.65        54\n",
            "\n",
            "    accuracy                           0.75       154\n",
            "   macro avg       0.73      0.73      0.73       154\n",
            "weighted avg       0.76      0.75      0.75       154\n",
            "\n",
            "Testing accuracy: 0.7532467532467533\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.82      0.81       101\n",
            "           1       0.63      0.58      0.61        53\n",
            "\n",
            "    accuracy                           0.74       154\n",
            "   macro avg       0.71      0.70      0.71       154\n",
            "weighted avg       0.74      0.74      0.74       154\n",
            "\n",
            "Validation accuracy: 0.7402597402597403\n",
            "\n",
            "--------------------------------------------------Post Pruning complete--------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.89      0.84       299\n",
            "           1       0.74      0.57      0.64       161\n",
            "\n",
            "    accuracy                           0.78       460\n",
            "   macro avg       0.77      0.73      0.74       460\n",
            "weighted avg       0.77      0.78      0.77       460\n",
            "\n",
            "Training accuracy: 0.7782608695652173\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.82       100\n",
            "           1       0.68      0.59      0.63        54\n",
            "\n",
            "    accuracy                           0.76       154\n",
            "   macro avg       0.74      0.72      0.73       154\n",
            "weighted avg       0.75      0.76      0.76       154\n",
            "\n",
            "Testing accuracy: 0.7597402597402597\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       101\n",
            "           1       0.73      0.62      0.67        53\n",
            "\n",
            "    accuracy                           0.79       154\n",
            "   macro avg       0.77      0.75      0.76       154\n",
            "weighted avg       0.79      0.79      0.79       154\n",
            "\n",
            "Validation accuracy: 0.7922077922077922\n"
          ]
        }
      ]
    }
  ]
}